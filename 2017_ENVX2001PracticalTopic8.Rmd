---
title: "Practical Topic 8"
author: "Willem Vervoort"
date: "`r Sys.Date()`"
output: 
    pdf_document:
      fig_width: 7
      fig_height: 6
      fig_caption: true
---
```{r setup, warning=F, message=F, echo=F}
# root dir
knitr::opts_knit$set(root.dir = 
                       "z:/willem/teaching/envx2001/otherdata")
library(pander)
library(knitr)
```

# Objectives

* Review linear regression by redoing simple linear regression
* Learn to use the correlation matrix as an exploratory data analysis tool;
* Learn to perform MLR and interpret the results using (i) R and (ii) Excel

**DATA:	Data_Topic9_2016.xls**

# EXERCISE 1 - PERFORMING SLR IN R looking at toxicity in peanuts

Data:  *Peanuts* worksheet

The data comprise, for 34 batches, the average level of the fungal contaminant aflatoxin in a sample of 120 pounds of peanuts and the percentage of non-contaminated peanuts in the whole batch. The data were collected with the aim of being able to predict the percentage of non-contaminated peanuts (‘percent’) from the aflatoxin level (‘toxin’) in a sample.

Read in the data using read.csv() after exporting the data as a csv file.
```{r LoadPeanutData}
Peanuts <- read.csv("Data_peanuts_Week8.csv")
pander(Peanuts[1:5,], caption="First 5 lines of the Peanuts data set")
```

a) In R make a scatter plot (using `plot(data$Toxin,data$Percent)`) of the data. Describe the relationship between the two variables. Would you say that the percentage of non-contaminated peanuts in a batch could be predicted accurately from the level of aflatoxin in a sample via a linear relationship?  
```{r scatterplot, fig.cap="Scatter plot of toxin and peanuts", echo=F, eval=F}
with(Peanuts,plot(Toxin,Percent))
# or
# plot(Peanuts$Toxin,Peanuts$Percent)
```

b)	Use simple linear regression (`lm()`) in R to fit a straight line to the data. What is the fitted model?  

```{r lmPeanuts, eval=F, echo=F}
# fit a linear model using lm()
mod <- lm(Percent~Toxin, data=Peanuts)
anova(mod)
summary(mod)

```

c)	Comment on the overall fit of the regression.  
d)	Is toxin a significant predictor of percentage non-contaminated peanuts?  
e)	Interpret the slope parameter in terms of quantifying the relationship between toxin and percent.  

# EXERCISE 2 – PERFORMING MLR IN R AND EXCEL

Data:  *Corn* worksheet, saved as *2017_CornData_Topic8.csv*
In this data

* y = P content of corn  
* x1 = inorganic P content of soil  
* x2 = organic P content of soil  
* n = 17 sites  

(The original data had 18 sites, one is removed here.)  

```{r LoadData}
Corn <- read.csv("2017_CornData_Topic8.csv")
pander(Corn[1:5,], caption="First 5 lines of the Corn data set")

```

## (i) Examination of correlations and significance
Some people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.

Instead we will examine the pairwise correlations to "get a feel" for the data.  and we will we will make a 3-dimensional surface plot using the package `lattice`.

Using R, we can calculate the correlation matrix quite easily. 
Note the use of `round()` to limit the number of significant digits.  
```{r}
round(cor(Corn),3)
```
Regrettably this does not tell us anything about the significance of the correlations. We can test the individual correlations by using the function `cor.test()`, but then we have to do each variable.  
```{r}
with(Corn,cor.test(CornP,InorgP))
```
These results tell you that the correlation between CornP and InorgP is significant and that the p-value for this is `r with(Corn,cor.test(CornP,InorgP))$p.value`.
To do all the correlations together use the function `rcorr()`, which is in the package `Hmisc`, but the data have to be a `matrix`. You might have to first install the package:  
```{r}
#install.packages("Hmisc")
require(Hmisc, quiet = T)
```

Then run:  
`rcorr(as.matrix(Corn))`  

### **Tasks:** 

1.  What do the results tell you? The top part repeats the correlations which you also got with `cor()`, but the bottom part gives the p-values. Which of the correlations are significant? If you would construct a regression model would you select both variables or just one?  
2.  If we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful? (Hint, the *r*^2^ is exactly that for a single predictor regression, the square of the correlation, *r*).  

### simple 3-D plot

A 3-D plot can be made using the function `levelplot()` in lattice.
Here we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.

```{r plot3D_data, fig.cap="A 3D plot of the Corn data"}
require(lattice, quiet = T)
levelplot(CornP ~ InorgP + OrgP, data = Corn, col.regions = topo.colors(100))
```

It is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don't have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.

We will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn. 

## (ii) MLR in R  

This is fairly simple and follows the same structure as simple linear regression and uses `lm()`.  

In this case:  
```{r}
MLR.Corn <- lm(CornP~InorgP + OrgP,data=Corn)
# run anova() to see the anova table
anova(MLR.Corn)
# run summary to see the parameter estimates
summary(MLR.Corn)
```

Firstly though, let's check the assumptions of regression are met via residual diagnostics. We demonstrated this in class, you can simply use `plot()` on the regression model object (`MLR.Corn`) to get the plots, but we limit ourselves to plot 1,2 and 5 in the output, which is defined in `which=c(1,2,5)`. We also plot the histogram of the resiudals using `hist(resid(MLR.Corn))`. The `par(mfrow=c(2,2))` simply splits the plot into 4 components and allows you to plot everything together. `par(mfrow=c(1,1))` puts the default back  

```{r, fig.width=5,fig.height=6, fig.cap="Diagnostic plots for Corn data"}
par(mfrow=c(2,2))
plot(MLR.Corn,which=c(1,2,5))
hist(resid(MLR.Corn))
par(mfrow=c(1,1))
```

### **Task**
Are there any apparent problems with normality of CornP residuals or equality of variance for this small data set?

## (iii) MLR in Excel  

(remember to activate the Data Analysis add in).
Excel requires each predictor variable to be side by side. The following screen capture requests a bivariate regression (predictors in columns B and C) with residual plots and tests of normality.  

![MLR in Excel.](../ENVX2001-code/MLR_in_EXCEL.jpg)

The output is as follows (we are not showing the plots however):

![MLR output in Excel.](../ENVX2001-code/MLR_ExcelOutput.png)

******

# EXERCISE 3 – THE CORRELATION MATRIX AS A TOOL FOR EXPLORATORY DATA ANALYSIS

Data:  *Loyn* worksheet, *2017_Loyn.csv*

This dataset is from Loyn (1987) which we are using in the lectures.  Fragmentation of forest habitat has an impact of wildlife abundance.  This study looked at the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.  The predictor variables are:

•	Altitude (m) [ALT]*
•	Year when the patch was isolated (years) [YRS.ISOL]
•	Grazing (coded 1-5 which is light to heavy) [GRAZE] 
•	Patch area (ha) [AREA]
•	Distance to nearest patch (km) [DIST]
•	Distance to largest patch (km) [LDIST]

* The name is [ ] is the one used in the Excel worksheet.

In this exercise we will focus on 2 predictors (YRS.ISOL and AREA).  Bring these and the response (ABUND) into R by exporting to a csv file and reading into R using read.csv().

```{r readDataLoyn, eval=F, echo=F}
Loyn <- read.csv("2017_Loyn.csv")
```

1.	Examine the histograms of each using `hist()` in R.  Comment on the assumptions for regression being met.
```{r histogramsLoyn, eval=F, echo=F}

par(mfrow=c(4,2))
hist(Loyn$ABUND)
hist(Loyn$ALT)
hist(Loyn$YRS.ISOL)
hist(Loyn$GRAZE)
hist(Loyn$AREA)
hist(Loyn$DIST)
hist(Loyn$LDIST)
par(mfrow=c(1,1))
```

2.	Calculate the correlation matrix using rcorr() from the Hmisc package or simply cor().  Are the predictors useful?

```{r corLoyn, eval=F, echo=F}
rcorr(Loyn)
```

3.	Examine the scatterplot matrix using pairs(). 
```{r pairsLoyn, eval=F, echo=F, fig.cap="Pairs plot Loyn data"}
pairs(Loyn)
```

4.	The AREA predictor has a small number of observations with very large values.  Apply a log10 transformation.  Why are you doing this?  
5.	Repeat steps (1) – (3) using the transformed value of AREA.
```{r TransfLoyn, eval=F, echo=F}
Loynlog <- Loyn
Loynlog$log10AREA <- log10(Loynlog$AREA)
Loynlog <- Loynlog[,-2]
hist(Loynlog$log10AREA)
rcorr(as.matrix(Loynlog))
pairs(Loynlog)
``` 

# EXERCISE 4 – MODELLING BIRD ABUNDANCE

Data:  *Loyn2* worksheet, *2017_Loyn2.csv*

This worksheet has 6 predictor variables but note the 3 new names [L10AREA, L10DIST, L10LDIST].  They are the log10 transformed versions of AREA, DIST and LDIST.  Each of these variables was log transformed as they had a small number of larger observations with high leverage.
Import into R using `read.csv()`

Use R for the following analysis

1.	Obtain the correlation between ABUND and all of the 6 predictor variables using `cor()`.  Based on these, what would you expect to be the best single predictor of ABUND?  
2.	Use multiple regression to see whether ABUND can be predicted from L10AREA and GRAZE.  Is there a significant relationship?  Are the assumptions met? We are using these 2 predictors as they have the largest absolute correlations. Use `lm()` and specify the model as `ABUND~L10AREA + GRAZE`.  
3.	How good is the model based on the (i) *r*^2^ (ii) adjusted *r*^2^? Use `summary()`.  
4.	Which variable(s) has the most significant effect(s)? (Refer specifically to the t probabilities in the table of predictors and their estimated parameters or coefficients in the output of `summary()`).  Interpret the p-values in terms of dropping predictor variables.  
5.	Repeat the multiple regression, but this time include YRS.ISOL as a predictor variable (it has the 3rd largest absolute correlation). This will allow you to assess the effect of YRS.ISOL with the other variables taken into account.  Compare the *r*^2^ and adjusted *r*^2^ values with those you calculated for the 2 predictor model.  
6.	Which is the better model?  Why?  
